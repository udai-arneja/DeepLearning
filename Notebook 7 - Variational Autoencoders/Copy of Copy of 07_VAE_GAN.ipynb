{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Copy of 07_VAE_GAN.ipynb","provenance":[{"file_id":"https://github.com/MatchLab-Imperial/deep-learning-course/blob/master/07_VAE_GAN.ipynb","timestamp":1615241217489}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3AFPgUAyk1BP","executionInfo":{"status":"ok","timestamp":1617044290797,"user_tz":-60,"elapsed":2373,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}}},"source":["#just run this\n","from google.colab import drive\n","import os\n","import json\n","from keras.callbacks import ReduceLROnPlateau\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import numpy as np\n","import keras \n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.utils import np_utils, to_categorical\n","import matplotlib.pyplot as plt\n","from keras.datasets import mnist\n","import tensorflow as tf\n","\n","\n","import numpy as np\n","from keras.applications.inception_v3 import InceptionV3, preprocess_input\n","from keras.applications.imagenet_utils import decode_predictions\n","from keras.preprocessing import image\n","from keras.datasets import mnist\n","from PIL import Image as pil_image\n","\n","\n","def inception_score(x, resizer=None, batch_size=32, denorm_im=1):\n","    r = None\n","    n_batch = (x.shape[0]+batch_size-1) // batch_size\n","    for j in range(n_batch):\n","        x_batch = x[j*batch_size:(j+1)*batch_size, :, :, :]\n","        if denorm_im:\n","          x_batch = (x_batch + 1)/2\n","        r_batch = inception_score_model.predict(x_batch) # r has the probabilities for all classes\n","        r = r_batch if r is None else np.concatenate([r, r_batch], axis=0)\n","    p_y = np.mean(r, axis=0) # p(y)\n","    e = r*np.log(r/p_y) # p(y|x)log(P(y|x)/P(y))\n","    e = np.sum(e, axis=1) # KL(x) = Î£_y p(y|x)log(P(y|x)/P(y))\n","    e = np.mean(e, axis=0)\n","    return np.exp(e) # Inception score\n","\n","\n","def image_inception_score(generator, n_ex=10000, dim_random=10, input_noise=None, denorm_im=1):\n","    if input_noise is None:\n","      input_noise = np.random.normal(0,1,size=[n_ex,dim_random])\n","    x_pred = generator.predict(input_noise)\n","    if len(x_pred.shape)==2:\n","      x_pred = x_pred.reshape(n_ex, 28, 28, 1)\n","    return inception_score(x_pred, denorm_im=denorm_im)\n","\n","\n","def plot_history(history, metric = None):\n","  # Plots the loss history of training and validation (if existing)\n","  # and a given metric\n","  # Be careful because the axis ranges are automatically adapted\n","  # which may not desirable to compare different runs.\n","  # Also, in some cases you may want to combine several curves in one\n","  # figure for easier comparison, which this function does not do.\n","  \n","  if metric != None:\n","    fig, axes = plt.subplots(2,1)\n","    axes[0].plot(history.history[metric])\n","    try:\n","      axes[0].plot(history.history['val_'+metric])\n","      axes[0].legend(['Train', 'Val'])\n","    except:\n","      pass\n","    axes[0].set_title('{:s}'.format(metric))\n","    axes[0].set_ylabel('{:s}'.format(metric))\n","    axes[0].set_xlabel('Epoch')\n","    fig.subplots_adjust(hspace=0.5)\n","    axes[1].plot(history.history['loss'])\n","    try:\n","      axes[1].plot(history.history['val_loss'])\n","      axes[1].legend(['Train', 'Val'])\n","    except:\n","      pass\n","    axes[1].set_title('Model Loss')\n","    axes[1].set_ylabel('Loss')\n","    axes[1].set_xlabel('Epoch')\n","  else:\n","    plt.plot(history.history['loss'])\n","    try:\n","      plt.plot(history.history['val_loss'])\n","      plt.legend(['Train', 'Val'])\n","    except:\n","      pass\n","    plt.title('Model Loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","\n","def train_epoch(gan, generator, discriminator, plt_frq=25, BATCH_SIZE=32, mnist=1):  \n","  vector_ind = np.random.permutation(x_train.shape[0])\n","  nb_epoch = int(x_train.shape[0]/BATCH_SIZE)\n","  pbar = tqdm_notebook(range(nb_epoch))\n","  for e in range(nb_epoch):  \n","    ind = vector_ind[e*BATCH_SIZE:(e+1)*BATCH_SIZE]\n","    # Make generative images\n","    image_batch = x_train[ind,:,:,:]    \n","    noise_gen = np.random.normal(0,1,size=[BATCH_SIZE,randomDim])\n","    generated_images = generator.predict(noise_gen)\n","    # Train discriminator on generated images\n","    X = np.concatenate((image_batch, generated_images))\n","    y = np.zeros([2*BATCH_SIZE])\n","    y[0:BATCH_SIZE] = 1\n","    y[BATCH_SIZE:] = 0\n","\n","    #make_trainable(discriminator,True)\n","    d_loss  = discriminator.train_on_batch(X,y)\n","    losses[\"d\"].append(d_loss)\n","    # train Generator-Discriminator stack on input noise to non-generated output class\n","    noise_tr = np.random.normal(0,1,size=[BATCH_SIZE,randomDim])\n","    y2 = np.zeros([BATCH_SIZE])\n","    y2[:] = 1\n","\n","    #make_trainable(discriminator,False)\n","    g_loss = gan.train_on_batch(noise_tr, y2 )\n","    losses[\"g\"].append(g_loss)\n","\n","    # Updates plots. This is a little bit of a mess due to how the notebook\n","    # handles the outputs\n","    if e % plt_frq==plt_frq-1:\n","      plot_loss(losses)\n","      plot_gen(mnist)\n","      fig, ax = plt.subplots(2,1, figsize=(20,10) )\n","      img=mpimg.imread('loss.png')\n","      ax[0].imshow(img)\n","      ax[0].axis('off')\n","      img=mpimg.imread('images.png')\n","      ax[1].imshow(img)\n","      ax[1].axis('off')\n","      plt.tight_layout()\n","      display.clear_output(wait=True)\n","      pbar.update(plt_frq)\n","      display.display(pbar)\n","      display.display(fig)\n","      plt.close()\n","      \n","def plot_loss(losses):\n","    plt.figure()\n","    plt.plot(losses[\"d\"], label='discriminitive loss')\n","    plt.plot(losses[\"g\"], label='generative loss')\n","    plt.legend()\n","    plt.savefig('./loss.png')\n","    plt.close()\n","def plot_gen(mnist=1, n_ex=16, dim=(4,4), figsize=(10,10)):\n","    noise = np.random.normal(0,1,size=[n_ex,randomDim])\n","    generated_images = generator.predict(noise)\n","    \n","    plt.figure(figsize=figsize)\n","    for i in range(generated_images.shape[0]):\n","        plt.subplot(dim[0],dim[1],i+1)\n","        if mnist:\n","          img = generated_images[i,:,:,0]\n","          plt.imshow(img, cmap='gray')\n","        else:\n","          img = generated_images[i,:,:,:]\n","          plt.imshow(img)\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.savefig('./images.png')\n","    plt.close()\n","\n","  "],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kFzQMMpok5B7","executionInfo":{"status":"ok","timestamp":1617014548674,"user_tz":-60,"elapsed":21246,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvIZT5RqqegbdTKMziVehjeBTFwnu84VVrx1gbS5A=s64","userId":"09880777762577870649"}},"outputId":"79e5644c-70d0-49d2-a876-11d29b0c75a3"},"source":["drive.mount('/content/drive')\n","os.chdir(\"/content/drive/My Drive/Deep Learning 2021/Notebook Seven - VAE GAN\")\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"," 07_VAE_GAN.ipynb  'Copy of Copy of 07_VAE_GAN.ipynb'  'Task One'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KcJpMf4_lKZf","executionInfo":{"status":"ok","timestamp":1617044296707,"user_tz":-60,"elapsed":1170,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}}},"source":["import numpy as np\n","import keras\n","np.random.seed(123)  # for reproducibility\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Dropout, Activation, Layer, Input, Lambda \n","from keras.layers import Multiply, Add, BatchNormalization, Reshape\n","from keras.layers import UpSampling2D, Convolution2D, LeakyReLU, Flatten, ReLU\n","\n","\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D \n","from keras.datasets import mnist\n","from keras import backend as K\n","from scipy.stats import norm\n","\n","import matplotlib.image as mpimg\n","import sys\n","\n","from tqdm import tqdm_notebook\n","from IPython import display\n","%matplotlib inline\n","\n","from keras import initializers\n","from keras.optimizers import Adam\n","from keras.layers.convolutional import Conv2D"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ajCmZ1gamBix","executionInfo":{"status":"ok","timestamp":1617044300930,"user_tz":-60,"elapsed":793,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}}},"source":["from keras.layers import Dropout, merge, concatenate, UpSampling2D, MaxPooling2D\n","from keras.layers import Conv2D, Dense, Reshape, Flatten, Activation\n","from keras.layers.normalization import BatchNormalization\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers import Conv2D, Input, Dense, Reshape, Flatten\n","from keras.optimizers import Adam\n","\n","def build_generator(im_shape):\n","  \n","  img_B = Input(shape=(im_shape[0], im_shape[1], 1))\n","  \n","  ## Encoder part\n","  conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(img_B)\n","  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","  conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n","  pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","  conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n","  pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","  conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n","  drop4 = Dropout(rate=0.5)(conv4)\n","  pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n","\n","  conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n","  drop5 = Dropout(rate=0.5)(conv5)\n","\n","  ## Now the decoder starts\n","  up6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n","  merge6 = concatenate([drop4,up6], axis = 3)\n","  conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n","\n","  up7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n","  merge7 = concatenate([conv3,up7], axis = 3)\n","  conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n","\n","  up8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n","  merge8 = concatenate([conv2,up8], axis = 3)\n","  conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n","\n","  up9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n","  merge9 = concatenate([conv1,up9], axis = 3)\n","  conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n","  conv10 = Conv2D(3, 3,  padding = 'same')(conv9)\n","\n","  model = Model(inputs = img_B, outputs = conv10, name='generator')\n","  \n","  return model"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"MlxWLF6EmEIA","executionInfo":{"status":"ok","timestamp":1617044303509,"user_tz":-60,"elapsed":879,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}}},"source":["def build_discriminator(im_shape):\n","  \n","  img_A = Input(shape=(im_shape[0], im_shape[1], 3))\n","  img_B = Input(shape=(im_shape[0], im_shape[1], 1))\n","\n","  combined_imgs = concatenate([img_A, img_B], axis=-1)\n","    \n","  disc_layer = Conv2D(64, kernel_size=(5, 5), padding='same', strides=(2, 2))(combined_imgs)\n","  disc_layer = LeakyReLU(alpha=0.2)(disc_layer)\n","  \n","  disc_layer = Conv2D(64, kernel_size=(5, 5), padding='same', strides=(2, 2))(disc_layer)\n","  disc_layer = BatchNormalization(momentum=0.8)(disc_layer)\n","  disc_layer = LeakyReLU(alpha=0.2)(disc_layer)\n","\n","  disc_layer = Conv2D(128, kernel_size=(5, 5), strides=(2, 2))(disc_layer)\n","  disc_layer = BatchNormalization(momentum=0.8)(disc_layer)\n","  disc_layer = LeakyReLU(alpha=0.2)(disc_layer)\n","  \n","  disc_layer = Flatten()(disc_layer)\n","  disc_layer = Dense(1024)(disc_layer)\n","\n","  prob = Dense(1, name=\"disc_dense\")(disc_layer)\n","\n","  discriminator = Model(inputs=[img_A, img_B], outputs=[prob], name='discriminator')\n","  \n","  return discriminator"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m7Y88qyslMqW","executionInfo":{"status":"ok","timestamp":1617014616657,"user_tz":-60,"elapsed":9350,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvIZT5RqqegbdTKMziVehjeBTFwnu84VVrx1gbS5A=s64","userId":"09880777762577870649"}},"outputId":"22df91be-7891-4c48-cc99-4a632a3bdffb"},"source":["#DATA\n","\n","original_dim = 784\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train.reshape(-1, original_dim) / 255.\n","x_test = x_test.reshape(-1, original_dim) / 255. \n","\n","#INCEPTION SCORE MODEL\n","\n","!wget https://imperialcollegelondon.box.com/shared/static/5cc14wf0s4qwj65lec5852jlmxfy32m9.h5 -O inception_score_mnist.h5\n","inception_score_model = keras.models.load_model('./inception_score_mnist.h5')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","--2021-03-29 10:43:31--  https://imperialcollegelondon.box.com/shared/static/5cc14wf0s4qwj65lec5852jlmxfy32m9.h5\n","Resolving imperialcollegelondon.box.com (imperialcollegelondon.box.com)... 103.116.4.197\n","Connecting to imperialcollegelondon.box.com (imperialcollegelondon.box.com)|103.116.4.197|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /public/static/5cc14wf0s4qwj65lec5852jlmxfy32m9.h5 [following]\n","--2021-03-29 10:43:31--  https://imperialcollegelondon.box.com/public/static/5cc14wf0s4qwj65lec5852jlmxfy32m9.h5\n","Reusing existing connection to imperialcollegelondon.box.com:443.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://imperialcollegelondon.app.box.com/public/static/5cc14wf0s4qwj65lec5852jlmxfy32m9.h5 [following]\n","--2021-03-29 10:43:31--  https://imperialcollegelondon.app.box.com/public/static/5cc14wf0s4qwj65lec5852jlmxfy32m9.h5\n","Resolving imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)... 103.116.4.201\n","Connecting to imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)|103.116.4.201|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://public.boxcloud.com/d/1/b1!R9putCAmvnDr2YGjFtAKkYBBwvivr63u5ECnh0LdRHjnvE9_i787M9_mwMMQtwmEY9ejH89BJ-ej5A-wAm1zX4ef0h_qNyzlJqxcVUVUTVBh0DsvHraZSM2LaujnilkoWud-aP9qVNm8oQJO7vPch28q0Bsux5AJr4egUd5jiV4Chf_PNv-fC3GpevT8B2pBwbJnqFNGfyRw_wqUIispwevYeOWZjY-qHNlwVFKuNYQXJuOS3FTMdJZh2b8VLiOAxglxZHzK66eif9sX3rni-ELOfKgQkMfEeZx37pbtissuSidRuokG_h7KuVm4l0T7pEtFGBBTHqVj6rsij1zvRtXfsJaV5PLKmCjZCMYsdnVOlFDPni39hlf-FEmyr9E6OfCbVsMCFvjystAxS4cizNjBJBZXB6ZoakVaWSX6mx1fJiTI-C1ALvGING7PwE6yK-WUQaHweBiL8ASMtf2sFajQxkvMpdpg8AJ645FLADSBpR5QA4BFX6Ec4PU0m1meYBs9Fqq9XX7a9dD6e163xZ8w0jSeZgChzywgBxBJIxl-qjyyYATtkPOVTjz4Y-1cDMv2Lg8qixkeq5-6MsgxriUbN_d7pm-mJlWyZ1xy_-UVDjnsui5m1lbsd19rownX16D4LlMF4vlfjOgqM5z311RyGdiHhhyC8kTgZKM0F340V4PH0ezd57Uti4P0da5iE7Fm9-0Zx3CYWqqQlBcHqMNn8oLZLBJsZE4ZoZi5_H6uire4wJVyvvfAPPlPjht5WYhTtGnImha_7g8JGy12UbcMtXjkoe4ytUYlM7MS3Fs30cWZSfIC7xodi1xBgtvq4sbp0A8iiUA-TYiPSaX8ls6SZbQOfwZDiFuntA_aCB_jmTiuvV4-0IivKUGe4DDUBtjgMZu5TDQHCZYg4NC57pagIxlLgxl6mlrwOmz4aj3P8LP91Tm2MLsH2WTnPUO55t5Xsyoz6CJ_bUeLw5OvANoVCq91xYvQ20CV-MzOvb-u9nkkt6lYAV_P7L4Vdq3ZeFJ5X9QBsRwka2t5zi-nSD2WfqVCGk4BjBBQFKE1gGkQ1f_vhfX6VuQPJMrWTbApPD5gbx9slXLzLDmpMmijuqgRYngWeN5sFa-QSe0wJ0ADlcm9rs5h3ysgUGfpMSIPlJX1HFHRbQZDyotw6UjMleK1koOlbJDKiLhg4fJIbZ2RRPjmwCvO05VzjVDQkIiEkq2rVGSrTd5z6Fp2bw4cgWfWSRgqlc9n8N4fdW04-xwiDsaZyVThQorxTdcAcyBEnx9JieuOW5azwU_CrnmQoqbrStNp7wR0rPK0Xjqr6trjWJsqEm7EFxpTYzb5xSQWAAEXPdLKHhgtwHqugvZV4F6oE2n3nRsaujAZl-fhf_kYynRQjhqZzP3ZOcCZ_t2MtKCjb5XlH7-GbGUV3xvHLa0./download [following]\n","--2021-03-29 10:43:32--  https://public.boxcloud.com/d/1/b1!R9putCAmvnDr2YGjFtAKkYBBwvivr63u5ECnh0LdRHjnvE9_i787M9_mwMMQtwmEY9ejH89BJ-ej5A-wAm1zX4ef0h_qNyzlJqxcVUVUTVBh0DsvHraZSM2LaujnilkoWud-aP9qVNm8oQJO7vPch28q0Bsux5AJr4egUd5jiV4Chf_PNv-fC3GpevT8B2pBwbJnqFNGfyRw_wqUIispwevYeOWZjY-qHNlwVFKuNYQXJuOS3FTMdJZh2b8VLiOAxglxZHzK66eif9sX3rni-ELOfKgQkMfEeZx37pbtissuSidRuokG_h7KuVm4l0T7pEtFGBBTHqVj6rsij1zvRtXfsJaV5PLKmCjZCMYsdnVOlFDPni39hlf-FEmyr9E6OfCbVsMCFvjystAxS4cizNjBJBZXB6ZoakVaWSX6mx1fJiTI-C1ALvGING7PwE6yK-WUQaHweBiL8ASMtf2sFajQxkvMpdpg8AJ645FLADSBpR5QA4BFX6Ec4PU0m1meYBs9Fqq9XX7a9dD6e163xZ8w0jSeZgChzywgBxBJIxl-qjyyYATtkPOVTjz4Y-1cDMv2Lg8qixkeq5-6MsgxriUbN_d7pm-mJlWyZ1xy_-UVDjnsui5m1lbsd19rownX16D4LlMF4vlfjOgqM5z311RyGdiHhhyC8kTgZKM0F340V4PH0ezd57Uti4P0da5iE7Fm9-0Zx3CYWqqQlBcHqMNn8oLZLBJsZE4ZoZi5_H6uire4wJVyvvfAPPlPjht5WYhTtGnImha_7g8JGy12UbcMtXjkoe4ytUYlM7MS3Fs30cWZSfIC7xodi1xBgtvq4sbp0A8iiUA-TYiPSaX8ls6SZbQOfwZDiFuntA_aCB_jmTiuvV4-0IivKUGe4DDUBtjgMZu5TDQHCZYg4NC57pagIxlLgxl6mlrwOmz4aj3P8LP91Tm2MLsH2WTnPUO55t5Xsyoz6CJ_bUeLw5OvANoVCq91xYvQ20CV-MzOvb-u9nkkt6lYAV_P7L4Vdq3ZeFJ5X9QBsRwka2t5zi-nSD2WfqVCGk4BjBBQFKE1gGkQ1f_vhfX6VuQPJMrWTbApPD5gbx9slXLzLDmpMmijuqgRYngWeN5sFa-QSe0wJ0ADlcm9rs5h3ysgUGfpMSIPlJX1HFHRbQZDyotw6UjMleK1koOlbJDKiLhg4fJIbZ2RRPjmwCvO05VzjVDQkIiEkq2rVGSrTd5z6Fp2bw4cgWfWSRgqlc9n8N4fdW04-xwiDsaZyVThQorxTdcAcyBEnx9JieuOW5azwU_CrnmQoqbrStNp7wR0rPK0Xjqr6trjWJsqEm7EFxpTYzb5xSQWAAEXPdLKHhgtwHqugvZV4F6oE2n3nRsaujAZl-fhf_kYynRQjhqZzP3ZOcCZ_t2MtKCjb5XlH7-GbGUV3xvHLa0./download\n","Resolving public.boxcloud.com (public.boxcloud.com)... 103.116.4.200\n","Connecting to public.boxcloud.com (public.boxcloud.com)|103.116.4.200|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1933896 (1.8M) [application/octet-stream]\n","Saving to: âinception_score_mnist.h5â\n","\n","inception_score_mni 100%[===================>]   1.84M  5.31MB/s    in 0.3s    \n","\n","2021-03-29 10:43:33 (5.31 MB/s) - âinception_score_mnist.h5â saved [1933896/1933896]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RcG_R3OAGe5m"},"source":["# Coursework\n"]},{"cell_type":"markdown","metadata":{"id":"tKkjwCF6uw3K"},"source":["## Task 2: Quantitative VS Qualitative Results\n","\n","In this task, we will observe the difference between two trained models for colouring images. One is the model trained during the tutorial, which uses a cGAN approach to predict the RGB pixel-wise values of a B&W image. The other one is a simple UNet autoencoder trained with a Mean Absolute Error (MAE) loss, which is trained to predict directly the RBG image without any GAN based learning strategy. We refer to the first and second models as cGAN and MAE models, respectively. For this task, 20 epochs trained weights for the cGAN and MAE models are provided. If desired, the code to train the MAE model can be found below:"]},{"cell_type":"code","metadata":{"id":"iCKFnMxkOMyS","colab":{"base_uri":"https://localhost:8080/","height":236},"executionInfo":{"status":"error","timestamp":1617014871212,"user_tz":-60,"elapsed":898,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvIZT5RqqegbdTKMziVehjeBTFwnu84VVrx1gbS5A=s64","userId":"09880777762577870649"}},"outputId":"d2df5dfd-0686-4be5-ed00-ede4c5d13ede"},"source":["# Input images and their conditioning images\n","\n","# Load the data, shuffled and split between train and test sets\n","dataset_loader = DataLoader(dataset_name = 'CIFAR10')\n","\n","training_shape = dataset_loader.get_dataset_shape()\n","test_shape = dataset_loader.get_dataset_shape(is_training=False)\n","\n","print('Shape of Training Images: {}'.format(training_shape))\n","print('Shape of Test Images: {}'.format(test_shape))\n","im_shape = (32, 32)\n","img_A = Input(shape=(im_shape[0], im_shape[1], 3))\n","img_B = Input(shape=(im_shape[0], im_shape[1], 1))\n","\n","# Build the generator\n","generator_mae = build_generator(im_shape)\n","fake_A = generator_mae(img_B)\n","generator_mae = Model(inputs=img_B, outputs=fake_A)\n","generator_mae.compile(optimizer = 'Adam', loss = 'mean_absolute_error', metrics = ['mae'])\n","\n","num_epochs = 1\n","batch_size = 128\n","n_batches = dataset_loader.get_num_batches(batch_size)\n","     \n","for epoch in range(num_epochs):\n","  \n","  start_time = datetime.datetime.now()\n","  \n","  # Record average losses. Monitorize the loss function. \n","  g_avg_loss = []\n","\n","  # load_batch() returns a batch generator\n","  # Before starting the epoch, it shuffles the dataset\n","  for batch_i, [imgs_A, imgs_B] in enumerate(dataset_loader.load_batch(batch_size)):\n","    \n","    # -----------------\n","    #  Train Generator (MAE)\n","    # -----------------\n","    g_loss = generator_mae.train_on_batch(imgs_B, imgs_A)\n","    g_avg_loss.append(g_loss[0])\n","\n","    fake_A = generator_mae.predict(imgs_B)  \n","\n","    elapsed_time = datetime.datetime.now() - start_time\n","    \n","    # Aproximation of epoch remaining time\n","    remaining_time = (elapsed_time/(batch_i+1)) * (n_batches-batch_i-1)\n","\n","    # Plot examples          \n","    if batch_i%50 == 0:\n","      showColoredIms(imgs_B, fake_A, imgs_A)      \n","    \n","    # Plot the progress\n","    if batch_i%10 == 0:          \n","\n","      print (\"[Epoch %d/%d] [Batch %d/%d] [G loss: %f] elapsed_time: %s  remaining_time: %s\" % (epoch, num_epochs,\n","              batch_i, n_batches, np.mean(g_avg_loss), elapsed_time, remaining_time))            \n","        \n","  # Saves optimizer and weights\n","  generator_mae.save('generator_mae.h5') "],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-ad2c31a6d044>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the data, shuffled and split between train and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'CIFAR10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtraining_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"l9aTGM7qwtyO"},"source":["Instead of training the models, we can directly load their pre-trained weights by running:"]},{"cell_type":"code","metadata":{"id":"BbVvCIQyw0B8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617044431529,"user_tz":-60,"elapsed":118326,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}},"outputId":"6de81564-e5e1-4881-833d-bacdfd9e1d1c"},"source":["!wget -O weights.zip https://imperialcollegelondon.box.com/shared/static/w2m93zcadbycpspaq4jifug9bo8l88ad.zip\n","!unzip -q ./weights.zip\n","!rm ./weights.zip\n","\n","!wget -O weights_mae.zip https://imperialcollegelondon.box.com/shared/static/empxfp2v05xp7h8rwsccuiy35c3vddzd.zip\n","!unzip -q ./weights_mae.zip\n","!rm ./weights_mae.zip\n","\n","generator_cGAN = keras.models.load_model('./weights/generator.h5')\n","generator_mae = keras.models.load_model('mae_generator.h5', compile=False)\n","\n","generator_cGAN.compile(optimizer='Adam', loss='mean_absolute_error', metrics=['mae'])\n","generator_mae.compile(optimizer='Adam', loss='mean_absolute_error', metrics=['mae'])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["--2021-03-29 18:58:34--  https://imperialcollegelondon.box.com/shared/static/w2m93zcadbycpspaq4jifug9bo8l88ad.zip\n","Resolving imperialcollegelondon.box.com (imperialcollegelondon.box.com)... 185.235.236.197\n","Connecting to imperialcollegelondon.box.com (imperialcollegelondon.box.com)|185.235.236.197|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /public/static/w2m93zcadbycpspaq4jifug9bo8l88ad.zip [following]\n","--2021-03-29 18:58:34--  https://imperialcollegelondon.box.com/public/static/w2m93zcadbycpspaq4jifug9bo8l88ad.zip\n","Reusing existing connection to imperialcollegelondon.box.com:443.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://imperialcollegelondon.app.box.com/public/static/w2m93zcadbycpspaq4jifug9bo8l88ad.zip [following]\n","--2021-03-29 18:58:34--  https://imperialcollegelondon.app.box.com/public/static/w2m93zcadbycpspaq4jifug9bo8l88ad.zip\n","Resolving imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)... 185.235.236.201\n","Connecting to imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)|185.235.236.201|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://public.boxcloud.com/d/1/b1!6OJZUSdi9whC6iRHmA-iLL15yb5yfSdR0NjLHwx8peLG_-U4Kcf1Lqr_WjZjAXJifxgYIfDyc_GKvjn7g5PgG30S5f3ZugkGstaggut5KYShIYcQtagH93ar8jMXaFJLBMB0fmACuhl_iLPoGvzwSLAv2-m_8LSQbl8q-KCkzDvr2nHW85SRX041UB5y2fYFMqzC9Wt22D5tfR7BqitPvDc-IwvisPJ0zF_bU4UVtL4yTQtvkGsE2fVAPSI2bmEEtEUM-2DGB-7HtHIB7J9J9q7vXO3xBmF7H2fjIR3zf2N70iDKKhqNt2HwOadAfPKkSuZNhtk6xpd13E_qAUhb1cFe8Kywb1BC98TPySL6QIzujaKVfV6PdS1rQiJPCY02Un1kHHQ1e7Bh6hs0KMXwY0EWX6fvM5wB6WPtSHERaAMGetDPmnGr3u25KaTZU9ASNutpCbvt3HmiZtxBAiF9LH-O94KkuOqIIRTqFJTrqE8U2f5JgdJoANWojdRxMxrGI0lxtgsjryEAnPi32Nh8Uo6x8CkntMW9DHHVWfGpiPPgC8kpb5qZWIdoVFdJOIcQoodBbJh_MCObCBUyp_C766IMBSldB9bFws62P2VVsYR4u-PzAOdP8zXHh3wrhw44IDEQhup0BzmGa5aG9v-qWqr7kkHNWrySrwC7YIiJt-eQYa3zKUs39Z2nuJz8JKiDiAKH_cVOOgESLlWVKS8xiTlnSaDsuJeAkSu4PjGrJ9pWhQg-2uJ3GXhy7EOgYRD-Z2lXtZU4hwt2tqxp2BOsrE2XXHClt5lpQLH8QQQppWerWnVwsDZ2cdHasXDGMsPaLVXYVMV9jVQgW4_-QjCpyVIHvVg9mIvsqjEeuHPp07fREEskNd-JF1-atpvlQnSd2qDzxoXerkdKtshzTUi9S-pD0sGt5RP6ZYc9krORatiGI3QDOCJOKXO_LgSinEnuFOsCC3ty9PudKg7JvyFf-kMF8SiPk0STDSytrfvL5mWwtrNDNwgQlFPGEomBfCPgAs098CxGa-UOLvCq_K6gi5VULlvIxkd_6AA85y1ytN0uwOCw9R8vOpDDB_xlOXXOgz6U6jyn7gpQ_C-3KIGL9C7RLCzQZmVk93bX6V3wXg1lH0M98QUz9r5LeDwUnydYt5pGtQnO8k5sdM812_-zqQ19HAh5XfChobQwuck5NX55KC6JtHfTAcA8_-tIMnhPYPLQPco94kW2nXbtOy88pDWiOkhYT2EANUMsMn44bYRurH5Wh4ZlGP1B0oV4ddmPzvqpgWeeC04c9y-j8JKmFNP0A-ZP88MXA3Uv9FL_JNN3aU4GZdfKtI7eBNO2cLC1i0aq4daUHvo_MQLIaK8Km7SIXD8p0ezvGsxCeY35qXIjsgsvbY07JstjJW9h_pXQhHNMU5leKszTqyMGNkf8BEtJnBjwuNlsnDogZ6V84MOX/download [following]\n","--2021-03-29 18:58:35--  https://public.boxcloud.com/d/1/b1!6OJZUSdi9whC6iRHmA-iLL15yb5yfSdR0NjLHwx8peLG_-U4Kcf1Lqr_WjZjAXJifxgYIfDyc_GKvjn7g5PgG30S5f3ZugkGstaggut5KYShIYcQtagH93ar8jMXaFJLBMB0fmACuhl_iLPoGvzwSLAv2-m_8LSQbl8q-KCkzDvr2nHW85SRX041UB5y2fYFMqzC9Wt22D5tfR7BqitPvDc-IwvisPJ0zF_bU4UVtL4yTQtvkGsE2fVAPSI2bmEEtEUM-2DGB-7HtHIB7J9J9q7vXO3xBmF7H2fjIR3zf2N70iDKKhqNt2HwOadAfPKkSuZNhtk6xpd13E_qAUhb1cFe8Kywb1BC98TPySL6QIzujaKVfV6PdS1rQiJPCY02Un1kHHQ1e7Bh6hs0KMXwY0EWX6fvM5wB6WPtSHERaAMGetDPmnGr3u25KaTZU9ASNutpCbvt3HmiZtxBAiF9LH-O94KkuOqIIRTqFJTrqE8U2f5JgdJoANWojdRxMxrGI0lxtgsjryEAnPi32Nh8Uo6x8CkntMW9DHHVWfGpiPPgC8kpb5qZWIdoVFdJOIcQoodBbJh_MCObCBUyp_C766IMBSldB9bFws62P2VVsYR4u-PzAOdP8zXHh3wrhw44IDEQhup0BzmGa5aG9v-qWqr7kkHNWrySrwC7YIiJt-eQYa3zKUs39Z2nuJz8JKiDiAKH_cVOOgESLlWVKS8xiTlnSaDsuJeAkSu4PjGrJ9pWhQg-2uJ3GXhy7EOgYRD-Z2lXtZU4hwt2tqxp2BOsrE2XXHClt5lpQLH8QQQppWerWnVwsDZ2cdHasXDGMsPaLVXYVMV9jVQgW4_-QjCpyVIHvVg9mIvsqjEeuHPp07fREEskNd-JF1-atpvlQnSd2qDzxoXerkdKtshzTUi9S-pD0sGt5RP6ZYc9krORatiGI3QDOCJOKXO_LgSinEnuFOsCC3ty9PudKg7JvyFf-kMF8SiPk0STDSytrfvL5mWwtrNDNwgQlFPGEomBfCPgAs098CxGa-UOLvCq_K6gi5VULlvIxkd_6AA85y1ytN0uwOCw9R8vOpDDB_xlOXXOgz6U6jyn7gpQ_C-3KIGL9C7RLCzQZmVk93bX6V3wXg1lH0M98QUz9r5LeDwUnydYt5pGtQnO8k5sdM812_-zqQ19HAh5XfChobQwuck5NX55KC6JtHfTAcA8_-tIMnhPYPLQPco94kW2nXbtOy88pDWiOkhYT2EANUMsMn44bYRurH5Wh4ZlGP1B0oV4ddmPzvqpgWeeC04c9y-j8JKmFNP0A-ZP88MXA3Uv9FL_JNN3aU4GZdfKtI7eBNO2cLC1i0aq4daUHvo_MQLIaK8Km7SIXD8p0ezvGsxCeY35qXIjsgsvbY07JstjJW9h_pXQhHNMU5leKszTqyMGNkf8BEtJnBjwuNlsnDogZ6V84MOX/download\n","Resolving public.boxcloud.com (public.boxcloud.com)... 185.235.236.200\n","Connecting to public.boxcloud.com (public.boxcloud.com)|185.235.236.200|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 66255745 (63M) [application/zip]\n","Saving to: âweights.zipâ\n","\n","weights.zip         100%[===================>]  63.19M  12.1MB/s    in 5.8s    \n","\n","2021-03-29 18:58:41 (10.8 MB/s) - âweights.zipâ saved [66255745/66255745]\n","\n","replace weights/discriminator.h5? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","replace weights/generator.h5? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","--2021-03-29 18:59:40--  https://imperialcollegelondon.box.com/shared/static/empxfp2v05xp7h8rwsccuiy35c3vddzd.zip\n","Resolving imperialcollegelondon.box.com (imperialcollegelondon.box.com)... 185.235.236.197\n","Connecting to imperialcollegelondon.box.com (imperialcollegelondon.box.com)|185.235.236.197|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /public/static/empxfp2v05xp7h8rwsccuiy35c3vddzd.zip [following]\n","--2021-03-29 18:59:40--  https://imperialcollegelondon.box.com/public/static/empxfp2v05xp7h8rwsccuiy35c3vddzd.zip\n","Reusing existing connection to imperialcollegelondon.box.com:443.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://imperialcollegelondon.app.box.com/public/static/empxfp2v05xp7h8rwsccuiy35c3vddzd.zip [following]\n","--2021-03-29 18:59:40--  https://imperialcollegelondon.app.box.com/public/static/empxfp2v05xp7h8rwsccuiy35c3vddzd.zip\n","Resolving imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)... 185.235.236.201\n","Connecting to imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)|185.235.236.201|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://public.boxcloud.com/d/1/b1!i1HimBMsecJpiRwCn3cYayZ3aXelQcoL97fASdbxxdms7esU54IvU59_HUNuiTJdOS1AwM1cS0-a9N6wkrV_tJIU76-llT6MKQJmzvuoKBtA69PzIQNFyuYcqyOhIQUZBu9Pjf148-DhxDpB2yGFqJSgILTrSFNvtYbOd00RUPwbSf7HwYsNsHNTRXfw3CS1ZNKp9oiovc1MB5vORsUGtzLBlBbZLeub55Yo0EOXquQhbs-o4FwT9Av0TSHgsYEClq6YWqoGDffM6hQH1FhDdiNOnLiSMmjOtin0ODh4tJ6Yln4-wMrCSRo08biveki2fweRJLPSa8vOjGSyAK30ftzRckeoHWw9an6hpdwwizHMFhRqPoXybTeYEmOAtJILCidRbS8fbuHlv62vJf2EbS5IZNpSp5ZdnONFEclut9Ig2BoAZ9pznwU88DwoGYBsDDJk-NYksqS4OCavgzzE58eznHmWLp5iPx5otLmMURL1A5q5ZxNqYElbxdoNnMu6cpitaXNUIM3MU7xoNjbgwdtb9HjMfDYwV_SeRX6SI8-KxoVZyUQK7fRB6-beABOihxXxgRnJYQFoZLOK3CStW-ALd_7D6dvhvpsQ-rh69l4egYGtJ99GCx7bi2o-Q9buAuoSPnZvwk9A-7RhhGldfa-jdcTVVG-A5kVXSuof_vF-pSAfVcfNopGVS3JlXCGnW1rkemLTg9aAksNxGFzHaQNfabhmqeFLx-W90PKWjzdFf-hZ0alAStJwW13aEoA4bMwGRWQ5Dw3k_Kfsb8zy-5wqbN1qn7DJ3hNEW8MVf0mc_9hNDmAl3WFJrjaPazjXZuElCCqgpF3YHG_ygW-xqSqJ9h4SzFG89dtJZgCQkou_hflX5wERh8rfY65HdWgHZ_ozoZeH84N6exz8e_wvlG1jQN9g7rTNDD7GCGQFUI4EhLI2o0qnNR_E1RXmk3pHW_0AxDZnsU76202boNQb2-DumKT_yJ6yA07UklknCu-4HWLHUjEqKV1BJBO8wIcS9dBjCvPQ9q47EzRNsvexSe7hVByZJC_-rqe_5g1gLVAq9hRxCyxeQmroGX7n2V-Gsr2N_GMTw2mrkV6XC-AxPdCMwoxyP2lZQy6RUP-hnbrH-B5dNPufp9HU6XZEA8cpGY6UocSBVekt1M5NGTNpHUzMDye32tnxmvboZ2fMF9WULxm1oIdn6WjNMxBtS3eHcosLdsmIj3YdYfHcNb9OfN4PnjIrzsdpyWyapbU7gHTjdbfe4h6s-eAYVN8dTRhu4l4dYioikG9_By84kNy4mYcbudxXqxjKwuEC33duWXRmnimh5tylClR3OjHTDUV7OiMd6TxSV9z0BYCdX1diCAawSlvPLgb491_nkdL1NAkd20z-XmQS9ipE8go1lwuRdjG5fUg4HkHAULfvh6NKrjsKig0R4o0IvnsDT-9ip6ihIpWDN7bakg../download [following]\n","--2021-03-29 18:59:41--  https://public.boxcloud.com/d/1/b1!i1HimBMsecJpiRwCn3cYayZ3aXelQcoL97fASdbxxdms7esU54IvU59_HUNuiTJdOS1AwM1cS0-a9N6wkrV_tJIU76-llT6MKQJmzvuoKBtA69PzIQNFyuYcqyOhIQUZBu9Pjf148-DhxDpB2yGFqJSgILTrSFNvtYbOd00RUPwbSf7HwYsNsHNTRXfw3CS1ZNKp9oiovc1MB5vORsUGtzLBlBbZLeub55Yo0EOXquQhbs-o4FwT9Av0TSHgsYEClq6YWqoGDffM6hQH1FhDdiNOnLiSMmjOtin0ODh4tJ6Yln4-wMrCSRo08biveki2fweRJLPSa8vOjGSyAK30ftzRckeoHWw9an6hpdwwizHMFhRqPoXybTeYEmOAtJILCidRbS8fbuHlv62vJf2EbS5IZNpSp5ZdnONFEclut9Ig2BoAZ9pznwU88DwoGYBsDDJk-NYksqS4OCavgzzE58eznHmWLp5iPx5otLmMURL1A5q5ZxNqYElbxdoNnMu6cpitaXNUIM3MU7xoNjbgwdtb9HjMfDYwV_SeRX6SI8-KxoVZyUQK7fRB6-beABOihxXxgRnJYQFoZLOK3CStW-ALd_7D6dvhvpsQ-rh69l4egYGtJ99GCx7bi2o-Q9buAuoSPnZvwk9A-7RhhGldfa-jdcTVVG-A5kVXSuof_vF-pSAfVcfNopGVS3JlXCGnW1rkemLTg9aAksNxGFzHaQNfabhmqeFLx-W90PKWjzdFf-hZ0alAStJwW13aEoA4bMwGRWQ5Dw3k_Kfsb8zy-5wqbN1qn7DJ3hNEW8MVf0mc_9hNDmAl3WFJrjaPazjXZuElCCqgpF3YHG_ygW-xqSqJ9h4SzFG89dtJZgCQkou_hflX5wERh8rfY65HdWgHZ_ozoZeH84N6exz8e_wvlG1jQN9g7rTNDD7GCGQFUI4EhLI2o0qnNR_E1RXmk3pHW_0AxDZnsU76202boNQb2-DumKT_yJ6yA07UklknCu-4HWLHUjEqKV1BJBO8wIcS9dBjCvPQ9q47EzRNsvexSe7hVByZJC_-rqe_5g1gLVAq9hRxCyxeQmroGX7n2V-Gsr2N_GMTw2mrkV6XC-AxPdCMwoxyP2lZQy6RUP-hnbrH-B5dNPufp9HU6XZEA8cpGY6UocSBVekt1M5NGTNpHUzMDye32tnxmvboZ2fMF9WULxm1oIdn6WjNMxBtS3eHcosLdsmIj3YdYfHcNb9OfN4PnjIrzsdpyWyapbU7gHTjdbfe4h6s-eAYVN8dTRhu4l4dYioikG9_By84kNy4mYcbudxXqxjKwuEC33duWXRmnimh5tylClR3OjHTDUV7OiMd6TxSV9z0BYCdX1diCAawSlvPLgb491_nkdL1NAkd20z-XmQS9ipE8go1lwuRdjG5fUg4HkHAULfvh6NKrjsKig0R4o0IvnsDT-9ip6ihIpWDN7bakg../download\n","Resolving public.boxcloud.com (public.boxcloud.com)... 185.235.236.200\n","Connecting to public.boxcloud.com (public.boxcloud.com)|185.235.236.200|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 170349881 (162M) [application/zip]\n","Saving to: âweights_mae.zipâ\n","\n","weights_mae.zip     100%[===================>] 162.46M  11.1MB/s    in 15s     \n","\n","2021-03-29 18:59:57 (10.6 MB/s) - âweights_mae.zipâ saved [170349881/170349881]\n","\n","replace mae_generator.h5? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YyXI9hCTzUn2"},"source":["We have loaded both models, and we are ready to compare them. In this task, you are asked to analyse the difference between the quantitative versus the qualitative results. To do so, we provided two pieces of code. The first one will compute the MAE metric for both models in the test dataset. As we know, this metric is widely used on image generation tasks, such as image upsampling, image reconstruction, image translation, and so on."]},{"cell_type":"code","metadata":{"id":"t46neFcx6nyg","colab":{"base_uri":"https://localhost:8080/","height":236},"executionInfo":{"status":"error","timestamp":1617044436083,"user_tz":-60,"elapsed":727,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}},"outputId":"ab37a49d-98fc-4eea-c613-74117dc45153"},"source":["g_mae_avg_mae = []\n","for batch_i, [imgs_A, imgs_B] in enumerate(dataset_loader.load_batch(128, is_training=False)):\n","    _, mae = generator_mae.evaluate(imgs_B, imgs_A, verbose=0)\n","    g_mae_avg_mae.append(mae)\n","\n","print(\"MAE (Trained MAE): {:.4f}\".format(np.mean(g_mae_avg_mae)))\n","\n","g_cgan_avg_mae = []\n","for batch_i, [imgs_A, imgs_B] in enumerate(dataset_loader.load_batch(128, is_training=False)):\n","    _, mae = generator_cGAN.evaluate(imgs_B, imgs_A, verbose=0)\n","    g_cgan_avg_mae.append(mae)\n","\n","print(\"MAE (Trained cGAN): {:.4f}\".format(np.mean(g_cgan_avg_mae)))"],"execution_count":9,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-afe70ce7c9e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mg_mae_avg_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimgs_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_B\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mg_mae_avg_mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmae\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'dataset_loader' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"ddZgs-Il8F9B"},"source":["The next piece of code will show coloured examples for both networks, so you can check them visually and discuss which model is better. First, we need to create an iterator object to go through the test dataset:"]},{"cell_type":"code","metadata":{"id":"nAVQ-Dyj8ECJ","executionInfo":{"status":"aborted","timestamp":1617044279209,"user_tz":-60,"elapsed":27590,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}}},"source":["iterator = iter(dataset_loader.load_batch(1, is_training=False))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NjTR797V8eys"},"source":["Run multiple examples so that you have a clear idea of how both methods differ."]},{"cell_type":"code","metadata":{"id":"E-FC55MDy096","colab":{"base_uri":"https://localhost:8080/","height":236},"executionInfo":{"status":"error","timestamp":1617044287044,"user_tz":-60,"elapsed":848,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}},"outputId":"e4824a14-3fe8-4734-854e-e6f628cc0e52"},"source":["# Load test example\n","[imgs_A, imgs_B] = next(iterator)\n","\n","# Generate predictions for both models\n","fake_A_cGAN = generator_cGAN.predict(imgs_B)\n","fake_A_MAE = generator_mae.predict(imgs_B)\n","\n","# Plot all images\n","showColored_two_models_Ims(imgs_B, fake_A_MAE, fake_A_cGAN, imgs_A)"],"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2d12ea211072>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load test example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mimgs_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_B\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Generate predictions for both models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfake_A_cGAN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_cGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'iterator' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"ss_1bTVE-Us_"},"source":["We showed that both models obtain a similar MAE value. If we would only take \n","into account the quantitative metric, as done in many scientific articles, we would say that the MAE model is better. However, in addition to the quantitative results, we need to analyse visually the results produced by the two networks to declare which is the best model.\n","\n","**Report**\n","\n","\n","*   Run the previous code to analyse several coloured images for both models. Based on previous results and linked to GAN theory, discuss from the numerical and visual perspective if both models are similar, or whether there is a better one. You can provide in the report visual examples together with their MAE values to support your arguments. The figure of this task can be included in the Appendix. Discussion still needs to go into the main text."]}]}