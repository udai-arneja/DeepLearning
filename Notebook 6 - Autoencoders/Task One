{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Task One","provenance":[{"file_id":"https://github.com/MatchLab-Imperial/deep-learning-course/blob/master/06_Autoencoders.ipynb","timestamp":1615241260608}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"DZZXPtbTUfkq","executionInfo":{"status":"ok","timestamp":1617030001822,"user_tz":-60,"elapsed":1385,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}}},"source":["from google.colab import drive\n","import os\n","import json\n","from keras.callbacks import ReduceLROnPlateau\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import numpy as np\n","import keras \n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.utils import np_utils, to_categorical\n","import matplotlib.pyplot as plt\n","from keras.datasets import mnist\n","import tensorflow as tf\n","\n","def plot_history(history, metric = None):\n","  # Plots the loss history of training and validation (if existing)\n","  # and a given metric\n","  # Be careful because the axis ranges are automatically adapted\n","  # which may not desirable to compare different runs.\n","  # Also, in some cases you may want to combine several curves in one\n","  # figure for easier comparison, which this function does not do.\n","  \n","  if metric != None:\n","    fig, axes = plt.subplots(2,1)\n","    axes[0].plot(history.history[metric])\n","    try:\n","      axes[0].plot(history.history['val_'+metric])\n","      axes[0].legend(['Train', 'Val'])\n","    except:\n","      pass\n","    axes[0].set_title('{:s}'.format(metric))\n","    axes[0].set_ylabel('{:s}'.format(metric))\n","    axes[0].set_xlabel('Epoch')\n","    fig.subplots_adjust(hspace=0.5)\n","    axes[1].plot(history.history['loss'])\n","    try:\n","      axes[1].plot(history.history['val_loss'])\n","      axes[1].legend(['Train', 'Val'])\n","    except:\n","      pass\n","    axes[1].set_title('Model Loss')\n","    axes[1].set_ylabel('Loss')\n","    axes[1].set_xlabel('Epoch')\n","  else:\n","    plt.plot(history.history['loss'])\n","    try:\n","      plt.plot(history.history['val_loss'])\n","      plt.legend(['Train', 'Val'])\n","    except:\n","      pass\n","    plt.title('Model Loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","\n","def predict_representation(model, data, layer_name='representation'):\n","  ## We form a new model. Instead of doing \\psi\\phi(x), we only take \\phi(x)\n","  ## To do so, we use the layer name\n","  intermediate_layer_model = Model(inputs=model.input,\n","                                   outputs=model.get_layer(layer_name).output)\n","  representation = intermediate_layer_model.predict(data)\n","  representation = representation.reshape(representation.shape[0], -1)\n","  return representation\n","\n","# representation = predict_representation(model, x_test)\n","\n","def plot_representation_label(representation, labels, plot3d=0):\n","  ## Function used to plot the representation vectors and assign different \n","  ## colors to the different classes\n","\n","  # First create the figure\n","  fig, ax = plt.subplots(figsize=(10,6))\n","  # In case representation dimension is 3, we can plot in a 3d projection too\n","  if plot3d:\n","    ax = fig.add_subplot(111, projection='3d')\n","    \n","  # Check number of labels to separate by colors\n","  n_labels = labels.max() + 1\n","  # Color map, and give different colors to every label\n","  cm = plt.get_cmap('gist_rainbow')\n","  ax.set_prop_cycle(color=[cm(1.*i/(n_labels)) for i in range(n_labels)])\n","  # Loop is to plot different color for each label\n","  for l in range(n_labels):\n","    # Only select indices for corresponding label\n","    ind = labels == l\n","    if plot3d:\n","      ax.scatter(representation[ind, 0], representation[ind, 1], \n","                 representation[ind, 2], label=str(l))\n","    else:\n","      ax.scatter(representation[ind, 0], representation[ind, 1], label=str(l))\n","  ax.legend()\n","  plt.title('Features in the representation space with corresponding label')\n","  plt.show()\n","  return fig, ax\n","\n","# plot_representation_label(representation, y_test)\n","def plot_recons_original(image, label, model, size_image=(28,28)):\n","  ## Function used to plot image x and reconstructed image \\psi\\phi(x)\n","  # Reshape (just in case) and predict using model\n","  if len(image.shape) == 1:\n","    image = image.reshape(1, -1)\n","  reconst_image = model.predict(image)\n","  # Evaluate MSE to also report it in the image\n","  mse = model.evaluate(image, image, verbose=0)\n","  # Create a figure with 1 row and 2 columns\n","  plt.subplots(1,2)\n","  # Select first image in the figure\n","  ax = plt.subplot(121)\n","  # Plot image x\n","  plt.imshow(image.reshape(size_image[0],size_image[1]), cmap='gray')\n","  # This removes the ticks in the axis\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  # Select second image in the figure\n","  ax = plt.subplot(122)\n","  # Plot reconstructed image\n","  plt.imshow(reconst_image.reshape(size_image[0],size_image[1]), cmap='gray')\n","  # This removes the ticks in the axis\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  # Set a title to the current axis (second figure)\n","  plt.title('MSE of {:.2f}'.format(mse[0]))\n","  plt.show()\n","\n","\n","def cluster_plot_data(representation, use_gmm=0, random_state=42, plot=1):\n","  from sklearn.cluster import KMeans\n","  from sklearn import mixture\n","\n","\n","  # Set number of clusters to 10\n","  n_clusters = 10\n","  if use_gmm:\n","    c_pred = mixture.GaussianMixture(n_components=n_clusters, covariance_type='full', random_state=random_state).fit_predict(representation)\n","  else:\n","    c_pred = KMeans(n_clusters=n_clusters, random_state=random_state).fit_predict(representation)\n","  if plot:\n","    _, ax = plt.subplots(1,1)\n","    # Color map, and give different colors to every label\n","    cm = plt.get_cmap('gist_rainbow')\n","    ax.set_prop_cycle(color=[cm(1.*i/(n_clusters)) for i in range(n_clusters)])\n","    # Loop is to plot different color for each label\n","    for c in range(n_clusters):\n","      # Only select indices for corresponding label\n","      ind = c_pred == c\n","      ax.scatter(representation[ind, 0], representation[ind, 1], label=str(c))\n","    ax.legend()\n","    plt.title('Clustered features in the representation space')\n","    plt.show()\n","  return c_pred\n","  "],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OKXMfcEYUiMz","executionInfo":{"status":"ok","timestamp":1617028840703,"user_tz":-60,"elapsed":21879,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}},"outputId":"18ee7a20-b919-4949-e0f2-68369b560483"},"source":["drive.mount('/content/drive')\n","os.chdir(\"drive/My Drive/Deep Learning 2021/Notebook Six - Autoencoders\")\n","!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"," 06_Autoencoders.ipynb\t'Task One'  'Task One.ipynb'  'Task Two.ipynb'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UbsaPjIpmUG7","executionInfo":{"status":"ok","timestamp":1617031682699,"user_tz":-60,"elapsed":1468,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}}},"source":["import numpy as np\n","import keras\n","from keras.models import Sequential, Model\n","from keras.layers import Dense\n","import matplotlib.pyplot as plt\n","from keras.datasets import mnist\n","\n","num_classes = 10\n","# the data, shuffled and split between train and test sets\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train= x_train.astype('float32')\n","x_test= x_test.astype('float32')\n","\n","x_train /= 255.\n","x_test /= 255.\n","\n","x_train = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\n","x_test = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\n"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4w3Zw02rsdMK"},"source":["# Coursework"]},{"cell_type":"markdown","metadata":{"id":"mrJfQmqvASYC"},"source":["## Task 1: Non-linear Transformations for Representation Learning\n","\n","PCA is a standard dimensionality reduction technique that uses a linear transformation. In this task we are going to define two autoencoders, one convolutional and one without using any convolutional layer, that are capable of learning a non-linear transformation to reduce the dimensionality of the input MNIST image, and we will compare those autoencoders to PCA. A way to evaluate the quality of the representations produced by both PCA and the autoencoder is to learn a classifier on top of those representations with reduced dimensionality. If the classifier has high accuracy, then the representations can be considered meaningful. In our case, we will use representations with dimensionality 10 and we will use those representations to train a linear classifier, which is defined in the code below. \n","\n","The given example architectures for both the non-convolutional and the convolutional autoencoder already produce, after training them, representations of similar quality as PCA. Modify the given architectures and try to increase the accuracy when training a linear classifier on top of the autoencoder representations. The code given below may help you understand the pipeline.\n","\n","As in past notebooks, treat the MNIST test set as your validation set. You can use any of the layers and techniques presented in past notebooks, the only constraints are that the non-convolutional autoencoder should not have any Conv2d layer, that the convolutional autoencoder should include Conv2d layers, and that the representation vector should have dimensionality 10. \n","\n","**Report**:\n","* Table with the accuracy of `classifier` (defined below) obtained with the representations from your two proposed  autoencoder architectures (non-convolutional and convolutional autoencoder) and also with PCA with 10 components in the training set and the validation set. Additionally, include in the table the MSE error in both training and validation set for your non-convolutional autoencoder, for your convolutional autoencoder and for the PCA method. State clearly your two final autoencoder architectures and discuss the results."]},{"cell_type":"markdown","metadata":{"id":"DUSZnVLp1kq8"},"source":["We will use MNIST for this task. First, we resize all the images to have a resolution of 32x32, which will make the definition of the convolutional autoencoder easier."]},{"cell_type":"code","metadata":{"id":"oVL-ur3qr74_","executionInfo":{"status":"ok","timestamp":1617031838312,"user_tz":-60,"elapsed":67042,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}}},"source":["import numpy as np\n","import keras\n","np.random.seed(123)  # for reproducibility\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Dropout, Activation, Conv2D,Reshape, Flatten, BatchNormalization, UpSampling2D, MaxPool2D\n","\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D \n","from keras.datasets import mnist, cifar10\n","from tensorflow.keras.layers.experimental import preprocessing\n","from skimage.transform import resize\n","\n","num_classes = 10\n","\n","# Use x_test as your validation set\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train= x_train.astype('float32')\n","x_test= x_test.astype('float32')\n","x_train /= 255.\n","x_test /= 255.\n","x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2],1)\n","x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2],1)\n","x_train_32 = np.zeros((x_train.shape[0], 32,32,1))\n","x_test_32 = np.zeros((x_test.shape[0], 32,32,1))\n","# We resize the images\n","for n in range(x_train.shape[0]):\n","    x_train_32[n,:,:,:] = resize(x_train[n,:,:,:], (32,32,1), anti_aliasing=True)\n","for n in range(x_test.shape[0]):\n","    x_test_32[n,:,:,:] = resize(x_test[n,:,:,:], (32,32,1), anti_aliasing=True)"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"IsX8jeNctsag","executionInfo":{"status":"ok","timestamp":1617033622991,"user_tz":-60,"elapsed":1105,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}}},"source":["def predict_representation(model, data, layer_name='representation'):\n","  ## We form a new model. Instead of doing \\psi\\phi(x), we only take \\phi(x)\n","  ## To do so, we use the layer name\n","  intermediate_layer_model = Model(inputs=model.input,\n","                                   outputs=model.get_layer(layer_name).output)\n","  representation = intermediate_layer_model.predict(data)\n","  representation = representation.reshape(representation.shape[0], -1)\n","  return representation\n","\n","# representation = predict_representation(model, x_test)"],"execution_count":62,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":340},"id":"7hZoIlTEfAio","executionInfo":{"status":"error","timestamp":1617033634200,"user_tz":-60,"elapsed":10676,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}},"outputId":"882c7233-e32f-4214-b339-7130865f4310"},"source":["#PCA\n","from sklearn.decomposition import PCA\n","!pip install MulticoreTSNE\n","from MulticoreTSNE import MulticoreTSNE as TSNE\n","\n","pca = PCA(n_components=10)\n","pca = pca.fit(x_train_32.reshape(x_train_32.shape[0], -1))\n","\n","## We compute the representations for the different methods\n","representation_pca_train = pca.transform(x_train_32.reshape(x_train_32.shape[0], -1))\n","representation_pca_test = pca.transform(x_test_32.reshape(x_test_32.shape[0], -1))\n","# predict_representation is defined at the beginning of this notebook\n","representation_auto_train = predict_representation(autoencoder, x_train_32)\n","representation_auto_test = predict_representation(autoencoder, x_test_32)\n","# representation_conv_auto_train = predict_representation(conv_autoencoder, x_train_32)\n","# representation_conv_auto_test = predict_representation(conv_autoencoder, x_test_32)\n","\n","# We compute the MSE for the PCA method\n","reconst_train = pca.inverse_transform(representation_pca_train).reshape(x_train_32.shape[0], 32,32,1)\n","train_mse_pca = ((reconst_train - x_train_32)**2).mean()\n","\n","reconst_test = pca.inverse_transform(representation_pca_test).reshape(x_test_32.shape[0], 32,32,1)\n","test_mse_pca = ((reconst_test - x_test_32)**2).mean()\n","# We print the MSE for PCA, which you need to include on the table\n","\n","print(\"Autoencoder MSE: x_train {:.4f} - x_test {:.4f}\".format(representation_auto_train, representation_auto_test))\n","print(\"Conv Autoecncoder MSE: x_train {:.4f} - x_test {:.4f}\".format(representation_conv_auto_train, representation_conv_auto_test))\n","\n","\n","num_classes = 10\n","\n","### Use always this linear classifier, do not modify it\n","classifier = Sequential()\n","classifier.add(Dense(10, activation='softmax'))\n","classifier.compile(loss='sparse_categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['acc'])\n","epochs = 30\n","history_pca_classifier = classifier.fit(representation_pca_train, y_train, batch_size=128,\n","          epochs=epochs, verbose=1, validation_data=(representation_pca_test,y_test))\n","  \n","print(\"PCA MSE: x_train {:.4f} - x_test {:.4f}\".format(train_mse_pca, test_mse_pca)) \n","print(\"Accuracy with classifier: \", history_pca_classifier.history['acc'][-1])\n"," \n","# representation = predict_representation(pca, x_test_32)\n","# plot_representation_label(representation, y_test)\n","\n","\n","tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=500,n_jobs=8)\n","representation_tsne = tsne.fit_transform(representation_pca_test)\n","plot_representation_label(representation_tsne, y_test)\n","\n","\n","# Select a random index from the test set\n","# ind = np.random.randint(x_test.shape[0] -  1)\n","# plot_recons_original(x_test[ind], y_test[ind], model)\n","\n","# ind = np.random.randint(x_test.shape[0] -  1)\n","## The function below is defined in the tutorial\n","# plot_recons_original(np.expand_dims(x_test_32[ind],0), y_test[ind], pca, size_image=(32,32))"],"execution_count":63,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: MulticoreTSNE in /usr/local/lib/python3.7/dist-packages (0.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from MulticoreTSNE) (1.19.5)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from MulticoreTSNE) (1.14.5)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->MulticoreTSNE) (2.20)\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-63-69bb54afcb18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# We print the MSE for PCA, which you need to include on the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Autoencoder MSE: x_train {:.4f} - x_test {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepresentation_auto_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepresentation_auto_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Conv Autoecncoder MSE: x_train {:.4f} - x_test {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepresentation_conv_auto_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepresentation_conv_auto_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to numpy.ndarray.__format__"]}]},{"cell_type":"markdown","metadata":{"id":"G7WLHdZ79Mzk"},"source":["You can modify the code below to define your autoencoder. You can use any layer you want apart from `Conv2D` layers for this autoencoder."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hB5ideAKjgTI","executionInfo":{"status":"ok","timestamp":1617031008347,"user_tz":-60,"elapsed":60457,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}},"outputId":"4540f51b-9800-40b4-bd71-92b9e8c17707"},"source":["### Modify the model here\n","autoencoder = Sequential()\n","autoencoder.add(Flatten(input_shape=(32,32,)))\n","\n","autoencoder.add(Dense(512, activation='relu'))\n","autoencoder.add(Dense(128, activation='relu'))\n","autoencoder.add(Dense(32, activation='relu'))\n","\n","\n","# The representation has dimensionality 10, do not change the dimensionality\n","autoencoder.add(Dense(10, name='representation'))\n","\n","autoencoder.add(Dense(32, activation='relu'))\n","autoencoder.add(Dense(128, activation='relu'))\n","autoencoder.add(Dense(512, activation='relu'))\n","\n","autoencoder.add(Dense(32*32))\n","autoencoder.add(Reshape((32,32,1)))\n","# autoencoder.summary()\n","\n","autoencoder.compile(loss='mean_squared_error',\n","              optimizer='adam',\n","              metrics=['mse'])\n","# You can modify the number of epochs or other hyperparameters\n","epochs = 20\n","history = autoencoder.fit(x_train_32, x_train_32, batch_size=128,\n","          epochs=epochs, verbose=1, validation_data=(x_test_32,x_test_32))\n","\n","representation_auto_train = predict_representation(autoencoder, x_train_32)\n","representation_auto_test = predict_representation(autoencoder, x_test_32)\n","\n","# Computing Autoencoder non CNN MSE\n","#reconst_train_nc = history1.inverse_transform(representation_auto_train).reshape(x_train_32.shape[0], 32,32,1)\n","#train_mse_nc = ((reconst_train_nc - x_test_32)**2).mean()\n","\n","#reconst_test_nc = history1.inverse_transform(representation_auto_test).reshape(x_test_32.shape[0], 32,32,1)\n","#test_mse_nc = ((reconst_test_nc - x_test_32)**2).mean()\n","#print(\"Auto MSE: x_train {:.4f} - x_test {:.4f}\".format(train_mse_nc, test_mse_nc))\n","\n","print('Train MSE obtained is {:.4f}'.format(history.history['mse'][-1]))\n","print('Test MSE obtained is {:.4f}'.format(history.history['val_mse'][-1]))\n","\n","num_classes = 10\n","\n","### Use always this linear classifier, do not modify it\n","classifier = Sequential()\n","classifier.add(Dense(10, activation='softmax'))\n","classifier.compile(loss='sparse_categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['acc'])\n","epochs = 30\n","history_auto_classifier = classifier.fit(representation_auto_train, y_train, batch_size=128,\n","          epochs=epochs, verbose=1, validation_data=(representation_auto_test,y_test))\n","print(\"Accuracy with Auto no CNN classifier: \", history_auto_classifier.history['acc'][-1])\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","469/469 [==============================] - 2s 4ms/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.0180 - val_mse: 0.0180\n","Epoch 2/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0157 - val_mse: 0.0157\n","Epoch 3/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0141 - val_mse: 0.0141\n","Epoch 4/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0133 - val_mse: 0.0133\n","Epoch 5/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0126 - val_mse: 0.0126\n","Epoch 6/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0122 - val_mse: 0.0122\n","Epoch 7/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0119 - val_mse: 0.0119\n","Epoch 8/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0117 - val_mse: 0.0117\n","Epoch 9/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0114 - val_mse: 0.0114\n","Epoch 10/20\n","469/469 [==============================] - 2s 3ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0112 - val_mse: 0.0112\n","Epoch 11/20\n","469/469 [==============================] - 2s 3ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0110 - val_mse: 0.0110\n","Epoch 12/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0109 - val_mse: 0.0109\n","Epoch 13/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0109 - val_mse: 0.0109\n","Epoch 14/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0106 - val_mse: 0.0106\n","Epoch 15/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0106 - val_mse: 0.0106\n","Epoch 16/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0105 - val_mse: 0.0105\n","Epoch 17/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0104 - val_mse: 0.0104\n","Epoch 18/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0104 - val_mse: 0.0104\n","Epoch 19/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0103 - val_mse: 0.0103\n","Epoch 20/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0101 - val_mse: 0.0101\n","Train MSE obtained is 0.0099\n","Test MSE obtained is 0.0101\n","Epoch 1/30\n","469/469 [==============================] - 1s 2ms/step - loss: 3.0163 - acc: 0.2508 - val_loss: 1.1742 - val_acc: 0.6569\n","Epoch 2/30\n","469/469 [==============================] - 1s 2ms/step - loss: 1.0432 - acc: 0.7115 - val_loss: 0.7440 - val_acc: 0.8370\n","Epoch 3/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.7047 - acc: 0.8352 - val_loss: 0.5710 - val_acc: 0.8823\n","Epoch 4/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.5564 - acc: 0.8727 - val_loss: 0.4784 - val_acc: 0.8973\n","Epoch 5/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.4727 - acc: 0.8886 - val_loss: 0.4214 - val_acc: 0.9084\n","Epoch 6/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.4243 - acc: 0.8986 - val_loss: 0.3819 - val_acc: 0.9143\n","Epoch 7/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.3877 - acc: 0.9043 - val_loss: 0.3540 - val_acc: 0.9182\n","Epoch 8/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.3577 - acc: 0.9109 - val_loss: 0.3327 - val_acc: 0.9204\n","Epoch 9/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.3362 - acc: 0.9155 - val_loss: 0.3165 - val_acc: 0.9233\n","Epoch 10/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.3150 - acc: 0.9206 - val_loss: 0.3037 - val_acc: 0.9248\n","Epoch 11/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.3073 - acc: 0.9215 - val_loss: 0.2937 - val_acc: 0.9262\n","Epoch 12/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2891 - acc: 0.9235 - val_loss: 0.2852 - val_acc: 0.9279\n","Epoch 13/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2836 - acc: 0.9252 - val_loss: 0.2782 - val_acc: 0.9269\n","Epoch 14/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2853 - acc: 0.9238 - val_loss: 0.2732 - val_acc: 0.9279\n","Epoch 15/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2727 - acc: 0.9277 - val_loss: 0.2682 - val_acc: 0.9291\n","Epoch 16/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2707 - acc: 0.9272 - val_loss: 0.2643 - val_acc: 0.9303\n","Epoch 17/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2666 - acc: 0.9289 - val_loss: 0.2600 - val_acc: 0.9310\n","Epoch 18/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2635 - acc: 0.9283 - val_loss: 0.2572 - val_acc: 0.9313\n","Epoch 19/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2565 - acc: 0.9306 - val_loss: 0.2549 - val_acc: 0.9308\n","Epoch 20/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2595 - acc: 0.9289 - val_loss: 0.2536 - val_acc: 0.9315\n","Epoch 21/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2560 - acc: 0.9282 - val_loss: 0.2512 - val_acc: 0.9320\n","Epoch 22/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2520 - acc: 0.9306 - val_loss: 0.2491 - val_acc: 0.9326\n","Epoch 23/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2533 - acc: 0.9307 - val_loss: 0.2485 - val_acc: 0.9321\n","Epoch 24/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2502 - acc: 0.9307 - val_loss: 0.2465 - val_acc: 0.9325\n","Epoch 25/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2533 - acc: 0.9303 - val_loss: 0.2457 - val_acc: 0.9328\n","Epoch 26/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2486 - acc: 0.9324 - val_loss: 0.2439 - val_acc: 0.9332\n","Epoch 27/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2434 - acc: 0.9323 - val_loss: 0.2435 - val_acc: 0.9326\n","Epoch 28/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2478 - acc: 0.9316 - val_loss: 0.2436 - val_acc: 0.9322\n","Epoch 29/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2421 - acc: 0.9317 - val_loss: 0.2424 - val_acc: 0.9329\n","Epoch 30/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2420 - acc: 0.9326 - val_loss: 0.2416 - val_acc: 0.9328\n","Accuracy with Auto no CNN classifier:  0.9324333071708679\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"58upUwtD9S7x"},"source":["You can modify the code below to define your convolutional autoencoder. For this autoencoder you need to include `Conv2D` layers in your design, but you can use any other layer too. We show an example of a simple convolutional architecture below"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"et_8ZMxYpTjL","executionInfo":{"status":"ok","timestamp":1617032649999,"user_tz":-60,"elapsed":130189,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}},"outputId":"b6b3ca60-da08-40c6-88c6-353c7647daa6"},"source":["### Modify the model here\n","from keras.layers import Input,Conv2D,MaxPooling2D,UpSampling2D, BatchNormalization, Dropout\n","### This is an example of a simple/bad convolutional autoencoder\n","conv_autoencoder = Sequential()\n","# conv_autoencoder.add(Conv2D(32, 3, strides=2, padding='same'))\n","\n","\n","\n","conv_autoencoder.add(MaxPooling2D())\n","conv_autoencoder.add(Conv2D(128, 3 , padding='same'))\n","conv_autoencoder.add(MaxPooling2D())\n","# conv_autoencoder.add(BatchNormalization())\n","conv_autoencoder.add(Conv2D(256, 3,  padding='same'))\n","# conv_autoencoder.add(BatchNormalization())\n","# conv_autoencoder.add(Dropout(0.3))\n","conv_autoencoder.add(MaxPooling2D())\n","conv_autoencoder.add(Conv2D(512, 3 , padding='same'))\n","conv_autoencoder.add(MaxPooling2D())\n","conv_autoencoder.add(Conv2D(512, 3 , padding='same'))\n","\n","\n","conv_autoencoder.add(Flatten())\n","# The representation has dimensionality 10, do not change the dimensionality\n","conv_autoencoder.add(Dense(10, name='representation'))\n","\n","\n","conv_autoencoder.add(Dense(1024))\n","conv_autoencoder.add(Activation('relu'))\n","# conv_autoencoder.add(Dropout(0.3))\n","conv_autoencoder.add(Dense(1024))\n","conv_autoencoder.add(Activation('relu'))\n","conv_autoencoder.add(Reshape((32,32,1)))\n","\n","conv_autoencoder.build((None,32,32,1))\n","conv_autoencoder.summary()\n","conv_autoencoder.compile(loss='mean_squared_error',\n","              optimizer='adam',\n","              metrics=['mse'])\n","# You can modify the number of epochs or other hyperparameters\n","epochs = 20\n","history2 = conv_autoencoder.fit(x_train_32, x_train_32, batch_size=128,\n","          epochs=epochs, verbose=1, validation_data=(x_test_32,x_test_32))\n","\n","representation_conv_auto_train = predict_representation(conv_autoencoder, x_train_32)\n","representation_conv_auto_test = predict_representation(conv_autoencoder, x_test_32)\n","\n","\n","print('Train MSE obtained is {:.4f}'.format(history2.history['mse'][-1]))\n","print('Test MSE obtained is {:.4f}'.format(history2.history['val_mse'][-1]))\n","\n","num_classes = 10\n","\n","### Use always this linear classifier, do not modify it\n","classifier = Sequential()\n","classifier.add(Dense(10, activation='softmax'))\n","classifier.compile(loss='sparse_categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['acc'])\n","epochs = 30\n","history_auto_cnn = classifier.fit(representation_conv_auto_train, y_train, batch_size=128,\n","          epochs=epochs, verbose=1, validation_data=(representation_conv_auto_test,y_test))\n","print(\"Accuracy with CNN auto classifier: \", history_auto_cnn.history['acc'][-1])"],"execution_count":54,"outputs":[{"output_type":"stream","text":["Model: \"sequential_42\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","max_pooling2d_39 (MaxPooling (None, 16, 16, 1)         0         \n","_________________________________________________________________\n","conv2d_55 (Conv2D)           (None, 16, 16, 128)       1280      \n","_________________________________________________________________\n","max_pooling2d_40 (MaxPooling (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","conv2d_56 (Conv2D)           (None, 8, 8, 256)         295168    \n","_________________________________________________________________\n","max_pooling2d_41 (MaxPooling (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","conv2d_57 (Conv2D)           (None, 4, 4, 512)         1180160   \n","_________________________________________________________________\n","max_pooling2d_42 (MaxPooling (None, 2, 2, 512)         0         \n","_________________________________________________________________\n","conv2d_58 (Conv2D)           (None, 2, 2, 512)         2359808   \n","_________________________________________________________________\n","flatten_28 (Flatten)         (None, 2048)              0         \n","_________________________________________________________________\n","representation (Dense)       (None, 10)                20490     \n","_________________________________________________________________\n","dense_89 (Dense)             (None, 1024)              11264     \n","_________________________________________________________________\n","activation_36 (Activation)   (None, 1024)              0         \n","_________________________________________________________________\n","dense_90 (Dense)             (None, 1024)              1049600   \n","_________________________________________________________________\n","activation_37 (Activation)   (None, 1024)              0         \n","_________________________________________________________________\n","reshape_30 (Reshape)         (None, 32, 32, 1)         0         \n","=================================================================\n","Total params: 4,917,770\n","Trainable params: 4,917,770\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/20\n","469/469 [==============================] - 6s 11ms/step - loss: 0.0446 - mse: 0.0446 - val_loss: 0.0152 - val_mse: 0.0152\n","Epoch 2/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0128 - val_mse: 0.0128\n","Epoch 3/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0120 - val_mse: 0.0120\n","Epoch 4/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0113 - val_mse: 0.0113\n","Epoch 5/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0109 - val_mse: 0.0109\n","Epoch 6/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0107 - val_mse: 0.0107\n","Epoch 7/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0105 - val_mse: 0.0105\n","Epoch 8/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0104 - val_mse: 0.0104\n","Epoch 9/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0102 - val_mse: 0.0102\n","Epoch 10/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0102 - val_mse: 0.0102\n","Epoch 11/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0100 - val_mse: 0.0100\n","Epoch 12/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0099 - val_mse: 0.0099\n","Epoch 13/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0099 - val_mse: 0.0099\n","Epoch 14/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0098 - val_mse: 0.0098\n","Epoch 15/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0096 - val_mse: 0.0096\n","Epoch 16/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0095 - val_mse: 0.0095\n","Epoch 17/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0096 - val_mse: 0.0096\n","Epoch 18/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0095 - val_mse: 0.0095\n","Epoch 19/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0094 - val_mse: 0.0094\n","Epoch 20/20\n","469/469 [==============================] - 5s 10ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0094 - val_mse: 0.0094\n","Train MSE obtained is 0.0088\n","Test MSE obtained is 0.0094\n","Epoch 1/30\n","469/469 [==============================] - 1s 2ms/step - loss: 3.0780 - acc: 0.1975 - val_loss: 1.0586 - val_acc: 0.6915\n","Epoch 2/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.8724 - acc: 0.7614 - val_loss: 0.5191 - val_acc: 0.8881\n","Epoch 3/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.4887 - acc: 0.8934 - val_loss: 0.3772 - val_acc: 0.9161\n","Epoch 4/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.3713 - acc: 0.9159 - val_loss: 0.3125 - val_acc: 0.9262\n","Epoch 5/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.3153 - acc: 0.9247 - val_loss: 0.2757 - val_acc: 0.9306\n","Epoch 6/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2751 - acc: 0.9308 - val_loss: 0.2521 - val_acc: 0.9334\n","Epoch 7/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2560 - acc: 0.9333 - val_loss: 0.2360 - val_acc: 0.9368\n","Epoch 8/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2425 - acc: 0.9347 - val_loss: 0.2248 - val_acc: 0.9385\n","Epoch 9/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2301 - acc: 0.9370 - val_loss: 0.2166 - val_acc: 0.9392\n","Epoch 10/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2217 - acc: 0.9377 - val_loss: 0.2105 - val_acc: 0.9406\n","Epoch 11/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2158 - acc: 0.9394 - val_loss: 0.2060 - val_acc: 0.9414\n","Epoch 12/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2075 - acc: 0.9406 - val_loss: 0.2030 - val_acc: 0.9419\n","Epoch 13/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2114 - acc: 0.9384 - val_loss: 0.2008 - val_acc: 0.9417\n","Epoch 14/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2130 - acc: 0.9376 - val_loss: 0.1989 - val_acc: 0.9429\n","Epoch 15/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2020 - acc: 0.9405 - val_loss: 0.1974 - val_acc: 0.9421\n","Epoch 16/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2050 - acc: 0.9392 - val_loss: 0.1970 - val_acc: 0.9428\n","Epoch 17/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2024 - acc: 0.9406 - val_loss: 0.1959 - val_acc: 0.9424\n","Epoch 18/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2012 - acc: 0.9415 - val_loss: 0.1954 - val_acc: 0.9434\n","Epoch 19/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1975 - acc: 0.9428 - val_loss: 0.1953 - val_acc: 0.9429\n","Epoch 20/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1990 - acc: 0.9421 - val_loss: 0.1947 - val_acc: 0.9430\n","Epoch 21/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2013 - acc: 0.9413 - val_loss: 0.1937 - val_acc: 0.9433\n","Epoch 22/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1935 - acc: 0.9426 - val_loss: 0.1938 - val_acc: 0.9433\n","Epoch 23/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2006 - acc: 0.9410 - val_loss: 0.1934 - val_acc: 0.9439\n","Epoch 24/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1977 - acc: 0.9430 - val_loss: 0.1936 - val_acc: 0.9433\n","Epoch 25/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1943 - acc: 0.9429 - val_loss: 0.1935 - val_acc: 0.9432\n","Epoch 26/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1985 - acc: 0.9417 - val_loss: 0.1931 - val_acc: 0.9433\n","Epoch 27/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1987 - acc: 0.9422 - val_loss: 0.1933 - val_acc: 0.9433\n","Epoch 28/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1973 - acc: 0.9411 - val_loss: 0.1933 - val_acc: 0.9437\n","Epoch 29/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1955 - acc: 0.9424 - val_loss: 0.1936 - val_acc: 0.9433\n","Epoch 30/30\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1899 - acc: 0.9443 - val_loss: 0.1932 - val_acc: 0.9436\n","Accuracy with CNN auto classifier:  0.9424833059310913\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G-bJ9xiNZzeA"},"source":["Below you have the code you will use to train the classifier. We first extract the representations using any of the two autoencoders we just trained or PCA and then we train the classifier on top, which is just a simple Dense layer. Better representations should make it easier for the given simple classifier to separate the classes and, therefore, have larger accuracy."]},{"cell_type":"code","metadata":{"id":"lUka6avw5Oh3","colab":{"base_uri":"https://localhost:8080/","height":433},"executionInfo":{"status":"error","timestamp":1617033093007,"user_tz":-60,"elapsed":11444,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}},"outputId":"d7dcb0b3-63a2-4f5f-e9be-256c5d59c800"},"source":["## We use tSNE for our dimensionality reduction technique so we can\n","## plot the features using a 2D plot as it leads to nice plots.\n","## However, tSNE is tricky to use as a general dimensionality reduction method\n","## for clustering due to issues mentioned here: https://distill.pub/2016/misread-tsne/\n","## TSNE: https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\n","## Nice article explaining shortcomings: https://distill.pub/2016/misread-tsne/\n","!pip install MulticoreTSNE\n","from MulticoreTSNE import MulticoreTSNE as TSNE\n","\n","## Use these parameters, the plots are highly dependent on perplexity value\n","tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=500,n_jobs=8)\n","representation_tsne = tsne.fit_transform(representation_auto_test)\n","plot_representation_label(representation_tsne, y_test)"],"execution_count":57,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: MulticoreTSNE in /usr/local/lib/python3.7/dist-packages (0.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from MulticoreTSNE) (1.19.5)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from MulticoreTSNE) (1.14.5)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->MulticoreTSNE) (2.20)\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-57-3d4cb2b82f1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m## Use these parameters, the plots are highly dependent on perplexity value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrepresentation_tsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepresentation_auto_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplot_representation_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepresentation_tsne\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/MulticoreTSNE/__init__.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, _y)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;31m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"kqqx1Q1WMCRY"},"source":["You can also check how the reconstructed images look with the autoencoders you just trained."]},{"cell_type":"code","metadata":{"id":"hEcnL2fMMBtP","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1617033096901,"user_tz":-60,"elapsed":1651,"user":{"displayName":"Udai Arneja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gitw9wgt1tkpkaA0M2DceBy82OW_PzJmQl0xko6PMI=s64","userId":"09880777762577870649"}},"outputId":"d08454b4-8c23-45a3-8a40-fccaf05072fb"},"source":["ind = np.random.randint(x_test.shape[0] -  1)\n","## The function below is defined in the tutorial\n","plot_recons_original(np.expand_dims(x_test_32[ind],0), y_test[ind], conv_autoencoder, size_image=(32,32))"],"execution_count":58,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[]}}]}]}